// Code generated by kafmesh-gen. DO NOT EDIT.

package details

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
	"time"

	"github.com/burdiyan/kafkautil"
	"github.com/lovoo/goka"
	"github.com/lovoo/goka/storage"
	"github.com/pkg/errors"
	"github.com/syncromatics/kafmesh/pkg/runner"
	"github.com/syndtr/goleveldb/leveldb/opt"
	"golang.org/x/sync/errgroup"

	"test/internal/kafmesh/models/testMesh/testId"
)

type TestToDatabase_ViewSource_Context interface {
	Context() context.Context
	Update(string, *testId.Test) error
}

type TestToDatabase_ViewSource interface {
	Sync(TestToDatabase_ViewSource_Context) error
}

type contextWrap_TestToDatabase struct {
	context context.Context
	job *runner.ProtoViewSourceJob
}

func (c *contextWrap_TestToDatabase) Context() context.Context {
	return c.context
}
func (c *contextWrap_TestToDatabase) Update(key string, msg *testId.Test) error {
	return c.job.Update(key, msg)
}

func Register_TestToDatabase_ViewSource(options runner.ServiceOptions, synchronizer TestToDatabase_ViewSource, updateInterval time.Duration, syncTimeout time.Duration) (func(context.Context) func() error, error) {
	brokers := options.Brokers
	var err error
	protoWrapper := options.ProtoWrapper
	codec, err := protoWrapper.Codec("testMesh.testId.test", &testId.Test{})
	if err != nil {
		return nil, errors.Wrap(err, "failed to create codec")
	}

	opts := &opt.Options{
		BlockCacheCapacity: opt.MiB * 1,
		WriteBuffer:        opt.MiB * 1,
	}

	path := filepath.Join("/tmp/storage", "viewSource", "testMesh.testId.test")

	err = os.MkdirAll(path, os.ModePerm)
	if err != nil {
		return nil, errors.Wrap(err, "failed to create view source db directory")
	}

	builder := storage.BuilderWithOptions(path, opts)
	view, err := goka.NewView(brokers,
		goka.Table("testMesh.testId.test"),
		codec,
		goka.WithViewStorageBuilder(builder),
		goka.WithViewHasher(kafkautil.MurmurHasher),
	)
	if err != nil {
		return nil, errors.Wrap(err, "failed creating synchronizer view")
	}

	e, err := goka.NewEmitter(brokers,
		goka.Stream("testMesh.testId.test"),
		codec,
		goka.WithEmitterHasher(kafkautil.MurmurHasher))

	if err != nil {
		return nil, errors.Wrap(err, "failed creating synchronizer emitter")
	}

	emitter := runner.NewEmitter(e)

	return func(outerCtx context.Context) func() error {
		return func() error {
			cancelableCtx, cancel := context.WithCancel(outerCtx)
			defer cancel()
			grp, ctx := errgroup.WithContext(cancelableCtx)

			timer := time.NewTimer(0)
			grp.Go(func() error {
				for {
					select {
					case <-ctx.Done():
						return nil
					case <-timer.C:
						select {
						case <-ctx.Done():
							return nil
						case <-view.WaitRunning():
						}
			
						newContext, cancel := context.WithTimeout(ctx, syncTimeout)
						c := runner.NewProtoViewSourceJob(newContext, view, emitter)
						cw := &contextWrap_TestToDatabase{newContext, c}
						err := synchronizer.Sync(cw)
						if err != nil {
							cancel()
							fmt.Printf("sync error '%v'", err)
							return err
						}
						err = c.Finish()
						if err != nil {
							cancel()
							fmt.Printf("sync finish error '%v'", err)
							return err
						}
						cancel()
						timer = time.NewTimer(updateInterval)
					}
				}
			})

			grp.Go(emitter.Watch(ctx))
			grp.Go(func() error {
				return view.Run(ctx)
			})

			select {
			case <- ctx.Done():
				err := grp.Wait()
				return err
			}
		}
	}, nil
}

